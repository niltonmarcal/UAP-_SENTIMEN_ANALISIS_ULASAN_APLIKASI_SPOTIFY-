{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjgRQF-1VmTc"
   },
   "source": [
    "UAP\n",
    "\n",
    "Nama: Nilton Is Marcal\n",
    "Nim: 202210370311181\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwb2MeDXxZBE"
   },
   "source": [
    "preprocessing_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1766202208846,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "UbBC_pACxZZG",
    "outputId": "fde68e4a-68c6-43bc-f852-0a8b56ff0876"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "DATA_PATH = \"scrapped_Spotify_ Putar Musik_ Podcast.csv\"\n",
    "OUT_PATH = \"spotify_uap_6000_balanced.csv\"\n",
    "TARGET_PER_CLASS = 2000  # 2000 negatif, 2000 netral, 2000 positif\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "# Ambil kolom penting\n",
    "df = df[[\"content\", \"score\"]].copy()\n",
    "df = df.rename(columns={\"content\": \"text\", \"score\": \"rating\"})\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "\n",
    "# Label 3 kelas dari rating\n",
    "def rating_to_label(r):\n",
    "    if pd.isna(r):\n",
    "        return np.nan\n",
    "    if r >= 4:\n",
    "        return \"positif\"\n",
    "    if r <= 2:\n",
    "        return \"negatif\"\n",
    "    return \"netral\"\n",
    "\n",
    "df[\"label\"] = df[\"rating\"].apply(rating_to_label)\n",
    "\n",
    "# Cleaning teks\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)      # hapus link\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)          # hapus simbol\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()          # rapikan spasi\n",
    "    return s\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Drop kosong & duplikat\n",
    "df = df.dropna(subset=[\"text_clean\", \"label\"])\n",
    "df = df.drop_duplicates(subset=[\"text_clean\"])\n",
    "\n",
    "print(\"Distribusi label (sebelum sampling):\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(\"Total bersih:\", len(df))\n",
    "\n",
    "# Balanced sampling 6000\n",
    "df_bal = (df.groupby(\"label\", group_keys=False)\n",
    "            .apply(lambda x: x.sample(n=TARGET_PER_CLASS, random_state=42)))\n",
    "df_bal = df_bal.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nDistribusi label (setelah sampling):\")\n",
    "print(df_bal[\"label\"].value_counts())\n",
    "print(\"Total balanced:\", len(df_bal))\n",
    "\n",
    "df_bal[[\"text_clean\", \"label\"]].to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7hHdruE2C2_"
   },
   "source": [
    "exploratory_data_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1766202212312,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "gURiEqom2Dst",
    "outputId": "3cff6d82-987f-4ecd-914b-f076946a3feb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = \"spotify_uap_6000_balanced.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Distribusi label\n",
    "counts = df[\"label\"].value_counts()\n",
    "plt.figure()\n",
    "counts.plot(kind=\"bar\")\n",
    "plt.title(\"Distribusi Label Sentimen\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Jumlah\")\n",
    "plt.show()\n",
    "\n",
    "# Panjang teks\n",
    "df[\"len\"] = df[\"text_clean\"].astype(str).apply(lambda x: len(x.split()))\n",
    "plt.figure()\n",
    "plt.hist(df[\"len\"], bins=30)\n",
    "plt.title(\"Histogram Panjang Teks (jumlah kata)\")\n",
    "plt.xlabel(\"Jumlah kata\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.show()\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldH59oIe2LTH"
   },
   "source": [
    "train_base_model_bilstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9401,
     "status": "ok",
     "timestamp": 1766202519147,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "b9k2IL3v2L-b",
    "outputId": "54d03d66-412f-4a90-8f0d-0b052b66a8ae"
   },
   "outputs": [],
   "source": [
    "import json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "DATA_PATH = \"spotify_uap_6000_balanced.csv\"\n",
    "MODEL_DIR = \"models/base_bilstm.keras\" # Added .keras extension\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df[\"text_clean\"].astype(str).values\n",
    "y = df[\"label\"].astype(str).values\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# Save label classes (buat streamlit)\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "with open(\"models/label_classes.json\", \"w\") as f:\n",
    "    json.dump(le.classes_.tolist(), f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Tokenizer Keras\n",
    "VOCAB_SIZE = 30000\n",
    "MAXLEN = 100\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_train), maxlen=MAXLEN, padding=\"post\"\n",
    ")\n",
    "X_test_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_test), maxlen=MAXLEN, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Save tokenizer (buat streamlit)\n",
    "with open(\"models/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 128, input_length=MAXLEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(MODEL_DIR)\n",
    "\n",
    "# Plot loss & accuracy (wajib modul)\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"BiLSTM Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"BiLSTM Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = np.argmax(model.predict(X_test_seq), axis=1)\n",
    "print(\"Classification Report (BiLSTM):\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix (BiLSTM):\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v4KBuFZ39Ls"
   },
   "source": [
    "train_pretrained_indobert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2c4f6bad62e64845a04c8005dd0c2a22",
      "903e71526d8e49258e5a0aeb53ff45e8",
      "4e8db10bd2084af488a0e5c75661f207",
      "c4a673ae20734fe49a9ef440ea7f624c",
      "59ba000b0f414ddcacc48791151d9d81",
      "f507f63dd9a04d4e8d7ae418af4c65d3",
      "364370891f634b868aea5dbc1b5e2345",
      "d791161f7c354e64a371367858c8d4fe",
      "f830143182fb4902a1d0481cb939e630",
      "5d647eb7a66c4feeb8d4e231f048d55a",
      "1ca0fe0f680e40df9777a669b76b148f",
      "c1075a7ce022438b99dafce6bf21725f",
      "c84c139e5c00443abf900599ae65f12a",
      "382813b89c3f4039ba033c3bf5575434",
      "1608f278e9974965ba43e64d07846bda",
      "154cb07921844afbac2df0bbf5c13272",
      "0fdc616b826042f5ac71dd948c3826ed",
      "d6fbffe6f9d24df680b36b8477aa56f9",
      "e6611c098c854f4aa1bc79e3069ef38c",
      "2237854873994284a57a789d61ac6288",
      "a77e225f92ea4df3ac21d01440257402",
      "0efa86dfe0db443d828181c917380cc8"
     ]
    },
    "executionInfo": {
     "elapsed": 391738,
     "status": "ok",
     "timestamp": 1766204540411,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "0QWiHdOQ4CPk",
    "outputId": "e1b9895c-167d-4c4e-ac01-49d0a9876bc3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!pip install -q evaluate\n",
    "!pip install -q --upgrade transformers datasets accelerate\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "DATA_PATH = \"spotify_uap_6000_balanced.csv\"\n",
    "OUT_DIR = \"models/indobert\"\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p1\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"text_clean\"].astype(str).tolist()\n",
    "labels = df[\"label\"].astype(str).tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "# Save label classes (buat streamlit & laporan)\n",
    "with open(\"models/label_classes.json\", \"w\") as f:\n",
    "    json.dump(le.classes_.tolist(), f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_ds  = Dataset.from_dict({\"text\": X_test,  \"label\": y_test})\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Test size:\", len(test_ds))\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "# -------------------------\n",
    "# TOKENIZE\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds  = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# -------------------------\n",
    "# MODEL\n",
    "# -------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(le.classes_)\n",
    ")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels_ = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels_)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels_, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# NOTE:\n",
    "# Kamu sebelumnya kena TypeError saat pakai evaluation_strategy/logging_strategy.\n",
    "# Di versi transformers baru, aman pakai eval_strategy & logging_strategy.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    eval_strategy=\"epoch\",          # <-- versi baru\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN\n",
    "# -------------------------\n",
    "trainer.train()\n",
    "\n",
    "# Save model & tokenizer\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# PREDICT & REPORT\n",
    "# -------------------------\n",
    "pred_out = trainer.predict(test_ds)\n",
    "y_pred = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "print(\"\\n=== Classification Report (IndoBERT) ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "print(\"=== Confusion Matrix (Raw) ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# -------------------------\n",
    "# CONFUSION MATRIX HEATMAP (KAYA CONTOH)\n",
    "# -------------------------\n",
    "def plot_confusion_matrix_heatmap(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(\n",
    "                j, i, f\"{cm[i, j]}\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.ylabel(\"Label Sebenarnya\")\n",
    "    plt.xlabel(\"Label Prediksi\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan gambar untuk laporan (opsional tapi bagus)\n",
    "    plt.savefig(\"cm_indobert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix_heatmap(\n",
    "    y_test, y_pred,\n",
    "    labels=le.classes_,\n",
    "    title=\"Confusion Matrix IndoBERT\"\n",
    ")\n",
    "\n",
    "print(\"Saved image: cm_indobert.png\")\n",
    "\n",
    "# -------------------------\n",
    "# LOSS PLOT (DARI LOG TRAINER)\n",
    "# -------------------------\n",
    "log_history = trainer.state.log_history\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_loss  = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_acc   = [x[\"eval_accuracy\"] for x in log_history if \"eval_accuracy\" in x]\n",
    "\n",
    "if train_loss:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, marker=\"o\")\n",
    "    plt.title(\"IndoBERT Training Loss (per epoch)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_indobert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved image: loss_indobert.png\")\n",
    "\n",
    "if eval_loss:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(eval_loss)+1), eval_loss, marker=\"o\")\n",
    "    plt.title(\"IndoBERT Eval Loss (per epoch)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eval_loss_indobert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved image: eval_loss_indobert.png\")\n",
    "\n",
    "if eval_acc:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(eval_acc)+1), eval_acc, marker=\"o\")\n",
    "    plt.title(\"IndoBERT Eval Accuracy (per epoch)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"acc_indobert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved image: acc_indobert.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIIFj7UWAGAI"
   },
   "source": [
    "train_pretrained_distilbert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8d288071bef64bed92277dcebb41e315",
      "bf0c73a00b6d4984a1e7563d2316b993",
      "374c5bb8e1e642c094362346c87e9af6",
      "45792a8d25d0424da225067e2e2dba3c",
      "1dd469d22fe244fda64fb7f8b97248b6",
      "bb8a050c50aa483aafb2988bae1107fc",
      "a045b1eba65f4055a5f47a8781f9eae5",
      "081e4abd32d94c19873dbebe996e1c13",
      "599ffcbb61564703b1e5368763b5f2dc",
      "3bae5387061f43a780ddb8c45e6a8891",
      "3a937f404cc54a629de5be0630e5f088",
      "9cf6325461974bf7ac653a428e72ea62",
      "78ed526b8c764ce882a08483ca260352",
      "7968fe04cd5642f998b1880ad29b4c4a",
      "9a281e19fc2c44ce91f37bd5db8d8313",
      "614d76ca49db47cb95ce83173a61656d",
      "f1a9dc50bda44c18b554a94bbb3dd957",
      "60edd953b45a40e7bdb4f4fb29f39f00",
      "20fbd6ee895c4ef7bd2578f11a08e3e1",
      "6fec0305ebae4c45b47a818a3f848de0",
      "3464c1ea6dfd49d3b3aea2d11758c8ab",
      "c8bbfa8c951b47fd8230fb375f4b846b"
     ]
    },
    "executionInfo": {
     "elapsed": 379503,
     "status": "ok",
     "timestamp": 1766205292302,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "y9IL8g0GAGzr",
    "outputId": "76c866eb-bb18-412d-ed61-00b28a2f8bdf"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q evaluate\n",
    "!pip install -q --upgrade transformers datasets accelerate\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "DATA_PATH = \"spotify_uap_6000_balanced.csv\"\n",
    "OUT_DIR = \"models/distilbert\"\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"text_clean\"].astype(str).tolist()\n",
    "labels = df[\"label\"].astype(str).tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "with open(\"models/label_classes.json\", \"w\") as f:\n",
    "    json.dump(le.classes_.tolist(), f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_ds  = Dataset.from_dict({\"text\": X_test,  \"label\": y_test})\n",
    "\n",
    "print(\"Train:\", len(train_ds), \"Test:\", len(test_ds))\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "# -------------------------\n",
    "# TOKENIZE\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds  = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# -------------------------\n",
    "# MODEL\n",
    "# -------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(le.classes_)\n",
    ")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels_ = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels_)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels_, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    eval_strategy=\"epoch\",      # âœ… FIX DI SINI\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN\n",
    "# -------------------------\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# PREDICTION & REPORT\n",
    "# -------------------------\n",
    "pred_out = trainer.predict(test_ds)\n",
    "y_pred = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "print(\"\\n=== Classification Report (DistilBERT) ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "print(\"=== Confusion Matrix (Raw) ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# -------------------------\n",
    "# CONFUSION MATRIX HEATMAP\n",
    "# -------------------------\n",
    "def plot_cm(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    ticks = np.arange(len(labels))\n",
    "    plt.xticks(ticks, labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, labels)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.ylabel(\"Label Sebenarnya\")\n",
    "    plt.xlabel(\"Label Prediksi\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cm_distilbert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_cm(y_test, y_pred, le.classes_, \"Confusion Matrix DistilBERT\")\n",
    "\n",
    "# -------------------------\n",
    "# LOSS & ACC PLOT\n",
    "# -------------------------\n",
    "log_hist = trainer.state.log_history\n",
    "train_loss = [x[\"loss\"] for x in log_hist if \"loss\" in x]\n",
    "eval_loss  = [x[\"eval_loss\"] for x in log_hist if \"eval_loss\" in x]\n",
    "eval_acc   = [x[\"eval_accuracy\"] for x in log_hist if \"eval_accuracy\" in x]\n",
    "\n",
    "if train_loss:\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, marker=\"o\")\n",
    "    plt.title(\"DistilBERT Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"loss_distilbert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "if eval_acc:\n",
    "    plt.figure()\n",
    "    plt.plot(eval_acc, marker=\"o\")\n",
    "    plt.title(\"DistilBERT Eval Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(\"acc_distilbert.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FDfutyJCpt4"
   },
   "source": [
    "model_evaluation_comparison.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1766205778232,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "LdqCWbEoCuOn",
    "outputId": "c9ce139a-1ba2-4077-dd77-200ac96468be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = [\n",
    "    {\"Model\": \"BiLSTM (Base)\", \"Accuracy\": 0.58, \"F1_Macro\": 0.59},\n",
    "    {\"Model\": \"IndoBERT\", \"Accuracy\": 0.67, \"F1_Macro\": 0.67},\n",
    "    {\"Model\": \"DistilBERT mBERT\", \"Accuracy\": 0.61, \"F1_Macro\": 0.61},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "# Simpan untuk README\n",
    "df.to_csv(\"model_comparison.csv\", index=False)\n",
    "print(\"Saved: model_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 309148,
     "status": "ok",
     "timestamp": 1766208369628,
     "user": {
      "displayName": "Nilenaaa.",
      "userId": "10352123195949440559"
     },
     "user_tz": -420
    },
    "id": "5U2iye_8FFch",
    "outputId": "99124dc7-cadd-4c6e-c28f-855a4e83be1c"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Nama file zip\n",
    "zip_name = \"models_uap_spotify.zip\"\n",
    "\n",
    "# Zip seluruh folder models\n",
    "shutil.make_archive(\"models_uap_spotify\", \"zip\", \"models\")\n",
    "\n",
    "# Download\n",
    "files.download(zip_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "INFILE = \"UAP_ML.ipynb\"      # pastikan nama file BENAR\n",
    "OUTFILE = \"UAP_ML_GITHUB.ipynb\"\n",
    "\n",
    "nb = nbformat.read(INFILE, as_version=4)\n",
    "\n",
    "# HAPUS TOTAL METADATA BERMASALAH\n",
    "nb.metadata = {}\n",
    "for cell in nb.cells:\n",
    "    cell.metadata = {}\n",
    "\n",
    "nbformat.write(nb, OUTFILE)\n",
    "print(\"FIXED NOTEBOOK SAVED AS:\", OUTFILE)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMxNZVNSmNLXFCCNz9gz72y",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
